\chapter{Evaluation}\label{chpt:evaluation}
\glsresetall

In order to test the application two neural networks were trained and evaluated. In this chapter the 
training settings as well as the results of the evaluation and the detection tests are discussed.

\section{Experiments}\label{sec:eval:experiments}

Both neural networks were trained on a dataset containing 1000 labeled images of the map 
"Ascent". The dataset has a train/validation/test split as described in 
section~\ref{subsec:app:dataset}. In each case 600 epochs and a batch size of 64 images were 
used. That is the highest possible batch size due to limited graphics memory of the used GPU. 
The small version (\texttt{yolov5s}) of the YOLOv5 model was deployed as network architecture. 

The first neural network (Model 1) was trained using the transfer learning concept. Starting 
point was the weights of the pre-trained \texttt{yolov5s} model. As described in 
section~\ref{subsec:app:training} the \texttt{freeze} option was applied to deactivate the adaptation 
of the backbone. Using the small YOLOv5 model ten layers had to be frozen. The number of 
backbone layers can be found in the network configuration file in the YOLOv5 
repository~\cite{jocher2020}. With this approach the training of the backbone only is done on the 
COCO dataset and so a huge part of the network is not refined on the actual data. Therefore a fine 
tuning task is applied, that trains the whole network for additional 100 epochs. In order to prevent 
major adjustments special fine tuning parameters including a small learning rate are used. The 
applied hyper parameters were defined by Ultralytics.

The second network (Model 2) serves as point of reference and was trained from scratch. Thereby it 
is necessary to provide a network configuration for the training script. That was done by using the 
configuration file of the \texttt{yolov5s} model and adapting the number of trained classes. In this 
case there are two classes: attacker and defender.

\section{Results}\label{sec:eval:results}

\begin{table}
	\centering
	\captionabove[Summary of metric results for the best epochs.]{Overview of metric results for the 
	weights from the best epoch.}
	\label{tab:eval:metricResultsBest}
	\begin{tabular}{p{0.03\textwidth}p{0.16\textwidth}p{0.02\textwidth}p{0.13\textwidth}p{0.03\textwidth}S[table-format=3.5]S[table-format=3.5]S[table-format=3.5]}
		\toprule
		&  \multicolumn{1}{c}{Training task} &&  \multicolumn{1}{c}{Epoch} && 
		\multicolumn{1}{c}{$mAP_{50}$} & 	\multicolumn{1}{c}{$mAP_{50-95}$} & 
		\multicolumn{1}{c}{$F1$}\\
		\midrule
		%
		\multicolumn{8}{l}{\bfseries{Model 1, Validation data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.988 & 0.664 & 
		0.978\\[5pt]
		&  \multicolumn{1}{c}{\textsc{finetune}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.995 & 0.825 
		& 0.99\\[7pt]
		%
		\multicolumn{8}{l}{\bfseries{Model 1, Test data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.991 & 0.661 & 
		0.975\\[5pt]
		&  \multicolumn{1}{c}{\textsc{finetune}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.994 & 0.819 
		& 0.986\\[7pt]
		%
		\multicolumn{8}{l}{\bfseries{Model 2, Validation data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.993 & 0.682 & 
		0.988\\[7pt]
		%
		\multicolumn{8}{l}{\bfseries{Model 2, Test data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.991 & 0.676 & 
		0.986\\
		%
		\bottomrule
	\end{tabular}
\end{table}

First of all, the implementation of the concept worked and the whole data pipeline from input video 
to output representation was able to be applied. The training task of the first model stopped 
automatically after 568~epochs because the best results were achieved in epoch 467~and in the 
following 100~epochs no further progress was done. This training task took around 2~hours and fine 
tuning was done in 45~minutes. In comparison to that training from scratch is more time-consuming: 
the network needed 4~hours and 20~minutes to complete the training of all 600~epochs. With 
regard to the time, transfer learning gives an advantage.

During training several configurations of the network weights are stored and in the end the training 
script is additionally saving the best one in a separate file. In table~\ref{tab:eval:metricResultsBest} 
results of different metrics for the best weight configurations are shown. Those metrics were 
introduced in section~\ref{subsec:intro:metrics}. The $F1$ score was calculated manually from 
precision and recall. The raw data that were recorded by the evaluation script are appended at the 
end of this report (Table~\ref{tab:ea:metricResults}). For the first model a distinction between the 
transfer learning task and the fine tuning is done in this data and all configurations are evaluated on 
the validation and on the test split. 

The evaluation of the mean average precision ($mAP$) and the $F1$ score shows that the fine 
tuning task of the first model is recommended because of a significant improvement of the 
performance in contrast to the plain transfer learning task. Considering only the $F1$ score 
depending on the data split the first model performs better than the model that was trained from 
scratch or they are equal. But relating to the $mAP_{50-95}$ the fine tuned model is the best 
working.

Therefore the best epoch of the fine tuned neural network was used to perform the detection tasks. 
The confidence and IoU threshold of the \gls{acr-nms} algorithm were set to respectively 0.7 during 
inference. In figure~\ref{fig:ea:outDetect} the result of an image from the map "Ascent" is shown. 
That image is not part of the used dataset and the bounding boxes were generated by the neural 
network with confidence values between 0.852 and 0.901. In a second step the model was fed with 
videos, starting with the trained map "Ascent". This worked very well, so also a recording of the map 
"Haven" was tested. The network has never learned about this map before but the results 
are quite good and are depicted in the figures~\ref{fig:app:output}, \ref{fig:ea:outputAtt} and 
\ref{fig:ea:outputComb}. This is a satisfying result because it shows a good generalization ability of 
the neural network even though it was not trained on "Haven" and only a 1000 image dataset was 
used. Challenging for the model is that the video of "Haven" shows a new background and there are 
other agents in the video with different images. That is the reason why the results are not perfect. 
Especially in the image of the attackers (see figure~\ref{fig:ea:outputAtt}) false predictions in form of 
single fragments at odd places can be seen but that could be improved with additional training on 
this map.
