\chapter{Evaluation}\label{chpt:evaluation}
\glsresetall

In order to test the application two neural networks were trained and evaluated. In this chapter the 
training settings as wall as the results of the evaluation and the detection tests are discussed.

\section{Experiments}\label{sec:eval:experiments}

Both neural networks were trained on a dataset containing 1000 labeled images of the map 
"Ascent". The dataset has a train/validation/test split as described in 
section~\ref{subsec:app:dataset}. In each case 600 epochs and a batch size of 64 images were 
used. That is the highest possible batch size due to limited graphics memory of the used GPU. As 
network architecture the small version (\texttt{yolov5s}) of the YOLOv5 model was deployed.

The first neural network (Model 1) was trained using the transfer learning concept. Starting 
point were the weights of the pre-trained \texttt{yolov5s} model. As described in 
section~\ref{subsec:app:training} the \texttt{freeze} option was applied to deactivate the adaptation 
of the backbone. Using the small YOLOv5 model ten layer had to be frozen. The number of 
backbone layers can be found in the network configuration file in the YOLOv5 
repository~\cite{jocher2020}. With this approach the training of the backbone only is done on the 
COCO dataset and so a huge part of the network is not refined on the actual data. Therefore a fine 
tuning task is applied, that trains the whole network for additional 100 epochs. In order to prevent 
major adjustments special fine tuning parameters including a small learning rate are used. The 
applied hyper parameters were defined by Ultralytics.

The second network (Model 2) serves as point of reference and was trained from scratch. Thereby it 
is necessary to provide a network configuration for the training script. That was done by using the 
configuration file of the \texttt{yolov5s} model and adapting the number of trained classes. In this 
case there are two classes (attacker and defender).

\section{Results}\label{sec:eval:results}

\begin{table}
	\centering
	\captionabove[Summary of metric results for the best epochs.]{Overview of metric results for the 
	weights from the best epoch.}
	\label{tab:eval:metricResultsBest}
	\begin{tabular}{p{0.03\textwidth}p{0.16\textwidth}p{0.02\textwidth}p{0.13\textwidth}p{0.03\textwidth}S[table-format=3.5]S[table-format=3.5]S[table-format=3.5]}
		\toprule
		&  \multicolumn{1}{c}{Training task} &&  \multicolumn{1}{c}{Epoch} && 
		\multicolumn{1}{c}{$mAP_{50}$} & 	\multicolumn{1}{c}{$mAP_{50-95}$} & 
		\multicolumn{1}{c}{$F1$}\\
		\midrule
		%
		\multicolumn{8}{l}{\bfseries{Model 1, Validation data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.988 & 0.664 & 
		0.978\\[5pt]
		&  \multicolumn{1}{c}{\textsc{finetune}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.995 & 0.825 
		& 0.99\\[7pt]
		%
		\multicolumn{8}{l}{\bfseries{Model 1, Test data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.991 & 0.661 & 
		0.975\\[5pt]
		&  \multicolumn{1}{c}{\textsc{finetune}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.994 & 0.819 
		& 0.986\\[7pt]
		%
		\multicolumn{8}{l}{\bfseries{Model 2, Validation data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.993 & 0.682 & 
		0.988\\[7pt]
		%
		\multicolumn{8}{l}{\bfseries{Model 2, Test data}} \\[7pt]
		&  \multicolumn{1}{c}{\textsc{train}} &&  \multicolumn{1}{c}{\textsc{best}} && 0.991 & 0.676 & 
		0.986\\
		%
		\bottomrule
	\end{tabular}
\end{table}

First of all the implementation of the concept worked and the whole data pipeline from input video to 
output representation can be applied. The training task of the first model stopped automatically after 
568 epochs because the best results were achieved in epoch 467 and in the following 100 epochs 
no further progress was made. This training task took around 2 hours and fine tuning was done in 45 
minutes. In comparison to that training from scratch is more time-consuming: the network 
needed 4 hours and 20 minutes to complete the training of all 600 epochs. With regard to the time, 
transfer learning gives an advantage.

During training several configurations of the network weights are stored and in the end the training 
script additionally saves the best one in a separate file. The results of the metrics introduced in 
section~\ref{subsec:intro:metrics} for the best configurations are listed in 
table~\ref{tab:eval:metricResultsBest}. The $F1$ score was calculated manually from precision 
and recall. All raw data that were recorded by the evaluation script are appended at the end of this 
report as an extended table containing data of several epochs (Table~\ref{tab:ea:metricResults}). 
For the first model a distinction between the transfer learning task and the fine tuning is made in this 
data and all configurations are evaluated on the validation and on the test split. 

The evaluation of the mean average precision ($mAP$) and the $F1$ score show that the fine tuning 
task of the first model is recommended because of a significant improvement of the performance. 
Considering only the $F1$ score depending on the data split the first model performs better than the 
model that was trained from scratch or they are equal. But relating to the $mAP_{50-95}$ the fine 
tuned model works best.

Therefore the best epoch of the fine tuned neural network was used to perform the detection tasks. 
The confidence and IoU threshold of the \gls{acr-nms} algorithm were set to respectively 0.7 during 
inference. In figure~\ref{fig:ea:outDetect} the result of an image from the map "Ascent" is shown. 
That image is not part of the used dataset and the bounding boxes were generated by the neural 
network with confidence values between 0.852 and 0.901. In a second step the model was fed with 
videos, starting with the trained map "Ascent". This worked very well, so also a recording of the map 
"Haven" was tested. The network has never learned about the map "Haven" before but the results 
are quite good. They are depicted in the figures~\ref{fig:app:output}, \ref{fig:ea:outputAtt} and 
\ref{fig:ea:outputComb}. This is a satisfying result because it shows a good generalization ability of 
the neural network. Challenging for the model is that the video of "Haven" shows a new background 
and there are other agents in the video with different images. Despite the good results especially in 
the image of the attackers (see figure~\ref{fig:ea:outputAtt}) false predictions in form of single 
fragments at odd places can be seen but that could be improved with additional training on this map.
