\chapter{Introduction}\label{chpt:introduction}
\glsresetall

E-Sports is a growing market for years. The events become more professional, prize money is rising 
and even broadcasters like ProSieben MAXX already show such events on 
television~\cite{kovacevic}. In light of this, the performance pressure of teams get bigger and 
preparation effort increases. But what exactly is e-sports? The German Games Industry Association 
defines the term e-sports as a competition between two or more people playing video games 
according to fixed rules~\cite{kovacevic}. Those people are mostly organized in professional teams 
from all around the world. Common game genre for e-sports events are real-time strategy games, 
multiplayer online battle arena games, tactical shooters and sport simulations. For popular games 
like League of Legends (Riot Games), Counter Strike: Global Offensive (Valve) or FIFA (Electronic 
Arts)  various leagues and tournaments were founded and the players have to show their best 
performances to keep their place in the team as professional gamer. During the corona pandemic 
online content streaming on platforms like Twitch or Mixer became very popular and also the 
popularity of e-sports was boosted, which often is streamed on such platforms~\cite{kovacevic}. In 
that time a new tactic shooter was published: Valorant from Riot Games~\cite{riotgames-valorant}. It 
became a big hype and therefore the player base grew very fast, so that Riot Games decided to start 
a tournament the "Valorant Champions Tour" also called VCT~\cite{riotgames-vct}. A concept and 
first implementation for a tool that can help Valorant teams and players to improve their performance 
in e.g. the highly competitive VCT is introduced in this project report.

\section{Problem Definition}\label{sec:intro:problem}

After that short introduction to e-sports in general, the following section defines the problem that is 
addressed within that project. The initial idea of that project came from a Youtube 
video~\cite{river2021} in that someone tried to implement an artificial intelligent agent that plays the 
video game Valorant autonomously.
Such agents in general are a big problem in the gaming scene because some players try to gain an 
unfair advantage from 3rd party programs. Those programs are used to display more information for 
the players than intended or helps them to react faster and more precise on occurring events than it 
is possible for a human being. Due to that problem it is prohibited in the terms and conditions of 
online multiplayer games to use any type of additional programs to play the game. Furthermore there 
are anti-cheat mechanisms running on the same computer as the game, searching for suspect 
programs that try to manipulate the outcome of a match. If a cheat program is detected the player 
becomes banned from the game. Because of that fact it is not that easy to implement and test an 
artificial agent that plays the game.

Due to cheating concerns the video and its content could be condemned but the creator just had 
educational purposes, did not harmed any player and in the end the implemented agent would not be 
good enough to compete with average Valorant players. In the video the creator found a way that he 
could implement an agent that cannot be detected by the anti-cheat mechanisms: he placed the 
agent on a separate computer than the game. Additionally in difference to programs that try to 
cheat, the artificial agent only became the ability to interact with the game client as every human 
player. Input was the content of the in-game screen, captured completely legal and output of the 
agent were mouse and keyboard commands, that were transmitted to the computer with the 
Valorant client running. With that setup he successfully created a system that was able to play the 
game on a very basic level. Due to the reason that he showed a way that could make it possible to 
build a cheat-client that cannot be detected by the anti-cheat of Valorant he decided not to share 
his source code~\cite{river2021}.

So the first idea of the project was to recreate a similar agent, but that would have been too much 
for a single project. Therefore the next idea was to build an application that is able to 
analyze Valorant gameplay and helps to evaluate already played games for training purposes, as it is 
common in most ordinary sport disciplines. The task of that application is to generate alternative 
representations of game data. Normally data from matches is available as recorded video from 
player respectively observer view or as raw data saved in databases from the game publisher. A 
transformation of those data can result in a better overview and understanding of the whole match 
and what happens there. In a first step those generated transformations can be analyzed by a trainer 
or the players themselves. Later it would be possible to use this data again as input for other pattern 
recognition algorithms that try to find problems in rounds that were lost and flaws in the tactics of 
opponents or even to invent new tactics. Further it could be possible to train an artificial agent with 
such transformations as input and try to reach a new level of tactical understanding.

As already mentioned one possibility to get game data are databases from the game publisher. 
There is also an \gls{acr-api} that would provide all data from matches but it is impossible to get an 
\gls{acr-api}-Key if it is not used for an already publicly running and from Riot Games legitimated 
application. For this reason the raw data will be gameplay videos that become transformed into a 
simpler form of presentation.

Summarized the task of that project is to use a pattern recognition algorithm to transform a 
gameplay video of the computer game Valorant into a more compact and easier to understand data 
representation. That includes acquisition and pre-processing of data; selection, training and 
evaluation of a neural network; choosing a trained version for inference and preparation of the 
output data for presentation.

\section{Valorant}\label{sec:intro:valorant}

Valorant is a character-based tactical shooter from the publisher Riot Games, Inc. an American 
video game developer based in Los Angeles, California. In that game two teams consisting of 
respectively five players are competing against each other, trying to win rounds. The first team 
winning 13 rounds wins the whole match. The game is played on different maps, that are modeling
fictitious towns. In the context of this project the maps "Ascent", "Haven" and "Split" are relevant, 
but in total there are currently ten maps. At the start of the game each player of a team chooses one 
out of 22 agents whereby every agent only can be chosen once per team. Those agents are 
categorized by four character classes: Duelist, Initiator, Sentinels and Controllers. The tactical 
meaning of that classes is not important for this project. Which team begins on attacker side and 
which starts as defender is decided randomly. After twelve rounds the sides are switching. The goal 
of the attacking team is to place a bomb (also called Spike) on certain areas on the map or to kill the 
defending team. For that the attackers have 100 seconds, otherwise the defenders win the round. Is 
the Spike planted a countdown of 45 seconds starts until detonation. In that time the defending team 
has the possibility to defuse the bomb and win the round for themselves. When the Spike explodes 
after the 45 seconds the attackers are winning the round. The last option for the defenders to win a 
round is to kill the whole attacking team and avoid that the bomb explodes if it was planted. To win a 
round each player can choose from a variety of weapons like in all shooter games. One special 
feature of Valorant is that every agent has a distinct set of abilities. Those abilities bring additional 
options to make an impact on the round~\cite{riotgames-valorant, spike2023, unrated2023}.

\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{images/02-ingame-minimap}
	\caption[In-game screenshot of Valorant]{In-game screenshot of Valorant with a marker on the 
	minimap.}
	\label{fig:intro:ingame}
\end{figure}

Figure~\ref{fig:intro:ingame} shows an in-game screen of Valorant from players perspective. Most 
important for this project is the blue marked part in the upper left corner. This is the minimap that 
gives an overview of the map\footnote{In that case it is the map "Ascent".} and where teammates or 
agent abilities are placed. In image~\ref{fig:intro:minimap} a zoomed version of this minimap can be 
seen. The gray area is an illustration of the map with the most distinctive points. Normally it is used 
for orientation. Furthermore on the minimap the teammates can be found, two of them are marked 
with a blue square. All five agents in that image are in the same team and on defender side. The side 
can be identified by the color "green" that surrounds the characters\footnote{Attackers would be 
pictured in red.}.  Except for the agent on the far left: that color is yellow, but that means that this is 
the agent whose perspective the screenshot is from, but he is also a defender. The item marked in 
yellow is an ability of an agent from the defender team. The red point marked with pink is an ability 
from an attacker/enemy. The two areas named "A" and "B" are the plant spots where the bomb can 
be placed as mentioned before.

\begin{figure}
	\centering
	\includegraphics[width=0.86\linewidth]{images/04-minimap-marker}
	\caption[Valorant minimap]{Valorant minimap with teammates (blue), team abilities (yellow) and 
	enemy ability (pink).}
	\label{fig:intro:minimap}
\end{figure}


\section{Related Work}\label{sec:intro:relatedWork}

In the following section related work of this project and basic knowledge is outlined.

\subsection[Video: Valorant AI]{Valorant AI}\label{subsec:intro:video}

First of all the Youtube video "I tried to make a Valorant AI using computer vision"~\cite{river2021} 
should be mentioned again, because it has provided the idea for this project and comes close to 
the work done within the scope of that project. The content of this video is already described in 
section~\ref{sec:intro:problem}. The video not only gave the idea but also shows some realization 
possibilities which have been used and adapted for this project. In particular how data could be 
created and that it uses YOLOv5~\cite{jocher2020} as a learning framework.

\subsection[Neural Network Framework: You Only Look Once]{You Only Look 
Once}\label{subsec:intro:yolo}

As mentioned before YOLOv5~\cite{jocher2020} was used to implement an artificial intelligent agent 
that is able to play the video game Valorant by itself. This is one major version of the neural network 
framework "You Only Look Once", in short YOLO. That framework is introduced in the subsequent 
part.

YOLO was initially published in 2016 by \citeauthor{redmon2016}~\cite{redmon2016}. Back then it 
was the first real-time end-to-end approach for object detection. In difference to other approaches it 
introduces a way to complete the detection task within a single pass of the network. This 
characteristic makes it possible to perform object detection as real-time application and is the 
reason why the framework is called YOLO\footnote{You Only Look Once}~\cite{terven2023}. 
Alternative frameworks like R-CNN~\cite{girshick1994} from \citeyear{girshick1994} or Fast 
R-CNN~\cite{girshick2015} from \citeyear{girshick2015} apply a two-stage object detection where 
regression is used to find object coordinates and classification to determine the probabilities. The 
YOLO framework uses regression for both tasks~\cite{aydin2023, terven2023}.

The development of YOLO continues to this day and miscellaneous authors have introduced 
enhanced versions of the framework. The versions YOLOv5~\cite{jocher2020} and 
YOLOv8~\cite{jocher2023}, for example, were developed by Ultralytics~\cite{ultralytics} with their 
founder and CEO Glen Jocher. While version eight is completely new from January 2023,  
YOLOv5 was published in 2020 but the latest release version 7.0 is also from 2023. Both 
frameworks are open source and actively maintained by Ultralytics and other 
contributors~\cite{terven2023}.

Despite various authors have developed the different versions of YOLO the fundamental concept of 
creating a fast and accurate framework was kept the same over all years. Therefore YOLO 
established itself as an easy to use, train and deploy neural network and is the most frequently used 
object detection algorithm~\cite{aydin2023, terven2023}. Because of the very good real-time object 
detection abilities the network is employed for autonomous vehicles, surveillance or sports 
analysis but it also has possible uses in agriculture or in the medical field~\cite{terven2023, 
zheng2022}. Finally it is worth mentioning, that there is also a mobile version of YOLO 
available~\cite{terven2023}.

\subsection{Metrics}\label{subsec:intro:metrics}

Consecutively metrics for evaluation of neural networks are introduced.

\subsubsection*{Precision and Recall}

Precision and recall are two metrics that evaluate the correct predictions of neural networks. While 
precision measures the accuracy of the positive predictions, recall quantifies the amount of correctly 
determined positive labels. They can be calculated as follows: 

\begin{equation}
	\label{eq:intro:precision}
	\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\begin{equation}
	\label{eq:intro:recall}
	\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

Thereby true positive ($TP$) refers to the number of positive predictions that are classified 
correctly. False positive ($FP$) predictions are incorrectly identified as positive and false negative 
($FN$) vice versa. In equation~\ref{eq:intro:precision} the set of $TP + FP$ describes all predictions 
that are assigned positive, while in the calculation of recall $TP + FN$ describes the set of ground 
truth (all positive samples)~\cite{terven2023, zheng2022}. 

For visualization precision and recall can be plotted in diagrams. To do so they are calculated 
for various confidence thresholds of the network's prediction. It is also possible to combine them 
afterwards in a joint precision-recall curve. When using a neural network there is always a 
trade-off between a high number of detected objects (high recall) and an increasing false positive 
rate (lower precision)~\cite{terven2023}.

\subsubsection*{Intersection over Union}

Another metric is \gls{acr-iou}. In object detection the goal is to accurately find the position of 
entities. A common way to do so is by predicting bounding boxes. \gls{acr-iou} is a way to rate the 
quality of those bounding boxes by comparing the predicted boxes with them from ground truth.

\begin{equation}
	\label{eq:intro:iou}
	\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}
\end{equation}    

\begin{figure}
	\centering
	\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
		\draw  [color={rgb, 255:red, 76; green, 119; blue, 230 }  ,draw opacity=1 ][line width=2.25]  
		(50,50) -- (100,50) -- (100,100) -- (50,100) -- cycle ;
		\draw  [color={rgb, 255:red, 251; green, 129; blue, 11 }  ,draw opacity=1 ][line width=2.25]  
		(80,60) -- (130,60) -- (130,110) -- (80,110) -- cycle ;
		\draw  [color={rgb, 255:red, 76; green, 119; blue, 230 }  ,draw opacity=1 ][line width=2.25]  
		(190,50) -- (240,50) -- (240,100) -- (190,100) -- cycle ;
		\draw  [color={rgb, 255:red, 251; green, 129; blue, 11 }  ,draw opacity=1 ][line width=2.25]  
		(200,60) -- (250,60) -- (250,110) -- (200,110) -- cycle ;
		\draw  [color={rgb, 255:red, 76; green, 119; blue, 230 }  ,draw opacity=1 ][line width=2.25]  
		(316,50) -- (366,50) -- (366,100) -- (316,100) -- cycle ;
		\draw  [color={rgb, 255:red, 251; green, 129; blue, 11 }  ,draw opacity=1 ][line width=2.25]  
		(312,54) -- (362,54) -- (362,104) -- (312,104) -- cycle ;
		
		\draw (51,22) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\text{IoU} = 0.32$};
		\draw (51,122) node [anchor=north west][inner sep=0.75pt]   [align=left] {Poor};
		
		\draw (191,21) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\text{IoU} = 0.71$};
		\draw (191,122) node [anchor=north west][inner sep=0.75pt]   [align=left] {Good};
		
		\draw (317,22) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\text{IoU} = 0.95$};
		\draw (316,122) node [anchor=north west][inner sep=0.75pt]   [align=left] {Excellent};
	\end{tikzpicture}
	\caption{Examples of different Intersection over Union (IoU) values.}
	\label{fig:intro:iou}
\end{figure}

In figure~\ref{fig:intro:iou} examples of different \gls{acr-iou} values and their correlating bounding 
boxes are depicted. The blue square always shows the ground truth label while the orange box is 
predicted by a neural network. As stated in equation~\ref{eq:intro:iou} the \gls{acr-iou} is calculated 
by the ratio of the overlapping or intersecting area to the union area~\cite{terven2023}.

\subsubsection*{Mean Average Precision}

\gls{acr-mAP}, also reffered to as Average Precision ($AP$) in literature, is a metric that is commonly 
used to evaluate the performance of object detection algorithms. It measures the average precision 
over all classes and is based on the precision-recall metric as well as on the
\gls{acr-iou}~\cite{terven2023}.

The calculation of the mean average precision starts with the computation of the precision-recall 
curve for each category. After that the category's \gls{acr-mAP} is calculated by averaging precision 
values from the precision-recall curve at multiple recall thresholds. Finally the overall mean average 
precision is derived by averaging the category's \gls{acr-mAP}. 

In order to get the common metric $mAP_{50}$ the same calculation is done under the 
constraint that the \gls{acr-iou} threshold is set to $0.5$. Calculating the $mAP_{50-95}$ is more 
complex because the category's \gls{acr-mAP} is not computed for just one \gls{acr-iou} threshold 
but for multiple in the range from $0.5$ to $0.95$ and step size $0.05$. At last the category's 
\gls{acr-mAP} is additionally averaged over the different \gls{acr-iou} thresholds and than the overall 
$mAP_{50-95}$ is calculated.

\subsubsection*{F1 Score}

The $F$ Score is a weighted combination of precision $P$ and recall $R$. As it can be seen in 
equation~\ref{eq:intro:fscore} a coefficient $\beta$ is applied to the precision and recall 
values~\cite{chaar2005, goutte2005, skopal2005}. 

\begin{equation}
	\label{eq:intro:fscore}
	F_\beta = \frac{(1 + \beta^2) \cdot P \cdot R}{(\beta^2 \cdot P) + R}
\end{equation} 

A commonly used metric is the $F1$ score, that is defined as the harmonic mean of precision and 
recall and can be obtained by choosing $\beta = 1$. Therefore the $F1$ score is a single ratio for 
evaluation of inference performance and is an attempt to find the best compromise between 
precision and recall. A Score of $F_1 = 1$ would be perfect~\cite{skopal2005}.

\begin{equation}
	\label{eq:intro:f1}
	F_1 = \frac{2 \cdot P \cdot R}{P + R}
\end{equation}   

\subsection{Transfer learning}\label{subsec:intro:transferlearning}

The transfer learning method was originally used by humans and is being transferred to machine 
learning. Thereby knowledge from related domains is used to learn the target domain in a more 
efficient way. That is particularly necessary when an insufficient amount of target training data is 
available due to data being rare or rather inaccessible or expensive to collect or label. In those cases 
existing datasets can be used to pre-train a machine learning model and after that a small number of 
target data is enough to transfer the model onto the target domain~\cite{weiss2016}.